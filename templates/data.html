<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="static/ai4ce.png" type="image/x-icon">
  <title>Spatial_reasoning_Data</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <style>
    /* Custom CSS styles */
    body {
      background-color: #f8f9fa;
    }

    .navbar-custom {
      background-color: #57068c;
    }

    .navbar-custom .navbar-brand {
      color: white;
      font-size: 24px;
    }

    .navbar-custom .navbar-nav .nav-link {
      color: white;
      font-size: 18px;
    }

    .navbar-custom .navbar-nav .nav-link:hover,
    .navbar-custom .navbar-nav .nav-link.active {
      background-color: #8e24aa;
    }

    .container {
      margin-top: 40px;
      background-color: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
    }

    .container h2,
    .container h3,
    .container p {
      margin-bottom: 20px;
    }
  </style>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      var currentPage = location.pathname;
      var navLinks = document.querySelectorAll(".navbar-nav .nav-link");
      navLinks.forEach(function(link) {
        var linkHref = link.getAttribute("href");
        if (linkHref && currentPage.startsWith(linkHref)) {
          link.classList.add("active");
        } else {
          link.classList.remove("active");
        }
      });
    });
  </script>
</head>
<body>
  <nav class="navbar navbar-expand-lg navbar-custom">
    <a class="navbar-brand" href="#">
      <img src="static/nyu-logo.jpg" alt="NYU Logo" height='60' width='60' class="d-inline-block align-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav"
      aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="/index">ABOUT</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/data">DATA</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/tutorial">TUTORIAL</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/methods">METHODS</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/ai4ce/Spatial_reasoning/tree/main">DOWNLOAD</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/aboutus">ABOUT US</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/visualize">VISUALIZE</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container">
    <h2>Data Sources:</h2>
    <p>The data for this project will be derived from two robust and widely used sources: Gibson Environment and Habitat-Sim.</p>
    <p>1. Gibson Environment: Gibsonâ€™s underlying database of spaces includes 572 full buildings composed of 1447 floors covering a total area of 211k m2s. The database is collected from real indoor spaces using 3D scanning and reconstruction. For each space, we provide: the 3D reconstruction, RGB images, depth, surface normal, and for a fraction of the spaces, semantic object annotations. In this page you can see various visualizations for each space, including 3D dissections, exploration using a randomly controlled husky agent, and standard point-to-point navigation episodes </p>
    <div style="display: flex; justify-content: center;">
      <img src="static/Gibson.png" alt="Gibson dataset" width="800" height="300">
    </div>
    <p>2. Habitat-Sim(High-Performance 3D Simulator): </p>
      <p>Supported Features:</p>
      <ul>
        <li>3D scans of indoor/outdoor spaces (with built-in support for HM3D, MatterPort3D, Gibson, Replica, and other datasets)</li>
        <li>CAD models of spaces and piecewise-rigid objects (e.g. ReplicaCAD, YCB, Google Scanned Objects)</li>
        <li>Configurable sensors (RGB-D cameras, egomotion sensing)</li>
        <li>Robots described via URDF (mobile manipulators like Fetch, fixed-base arms like Franka, quadrupeds like AlienGo)</li>
        <li>Rigid-body mechanics (via Bullet)</li>
      </ul>
    <p>These two sources will provide us with high-quality, realistic, and diverse data, thereby enabling a comprehensive investigation into spatial reasoning for RGB image matching. The combination of these datasets will also offer the opportunity to validate our method across a broad spectrum of scenarios, further establishing the generalizability and robustness of our proposed solution.</p>
  </div>

  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
</body>
</html>
