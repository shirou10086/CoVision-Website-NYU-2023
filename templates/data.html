<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="static/unkown.png" type="image/x-icon">
  <title>Co-VisiON_Data</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <style>
    /* Custom CSS styles */
    body {
      background-color: #f8f9fa;
    }

    .navbar-custom {
      background-color: #000000; /* unkown紫色 57068c*/
    }

    .navbar-custom .navbar-brand {
      color: white;
      font-size: 24px;
    }

    .navbar-custom .navbar-nav .nav-link {
      color: white;
      font-size: 18px;
    }

    .navbar-custom .navbar-nav .nav-link:hover,
    .navbar-custom .navbar-nav .nav-link.active {
      background-color: #8e24aa;
    }

    .container {
      margin-top: 40px;
      background-color: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
    }

    .container h2,
    .container h3,
    .container p {
      margin-bottom: 20px;
    }
  </style>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      var currentPage = location.pathname;
      var navLinks = document.querySelectorAll(".navbar-nav .nav-link");
      navLinks.forEach(function(link) {
        var linkHref = link.getAttribute("href");
        if (linkHref && currentPage.startsWith(linkHref)) {
          link.classList.add("active");
        } else {
          link.classList.remove("active");
        }
      });
    });
    function showAlert() {
        alert("Unable to provide download link till Official published");
    }
  </script>
</head>
<body>
  <nav class="navbar navbar-expand-lg navbar-custom">
    <a class="navbar-brand" href="#">
      <img src="static/unkown-logo.jpg" alt="unkown Logo" height='60' width='60' class="d-inline-block align-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav"
      aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="/index">ABOUT</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/data">DATA</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/tutorial">TUTORIAL</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/methods">METHODS</a>
        </li>
        <li class="nav-item">
            <a class="nav-link" href="#" onclick="showAlert()">DOWNLOAD</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/aboutus">ABOUT US</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/visualize">VISUALIZE</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container">
      <h2>Data Sources:</h2>
      <p>The data for this project will be derived from two robust and widely used sources: Gibson Environment and HM3D.</p>
      <p>1. Gibson Environment: The first half of our dataset is sampled from the Gibson dataset using the Habitat-sim platform. Gibson provides a collection of 3D models of real-world indoor environments, encompassing various types such as homes and offices. We chose the scenes based on the availability of .glb files and requisite metadata. In each scene, images are meticulously generated and strategically captured at specific positions and orientations such that they collectively maximize the coverage of unseen areas and minimize the overall image count. The image selection criterion is discussed in detail in Sec. A.1 and illustrated in Fig. 3. Depending on the scale of the indoor environments, the number of images ranges from a dozen to more than one hundred. To maintain the diversity and robustness of the dataset, we random seed numbers during the data generation, enabling us to collect a multitude of varied image sets that all fulfill the criteria of maximum coverage and minimum image count. A set of camera poses along with the resulting image set is referred to as a scenario of a scene. On average, approximately 14 images are taken per scenario for each of the 85 distinct scenes with a total of 129 floors, forming the current basis of the whole dataset for later co-visibility graph annotation. To calibrate the annotation procedure, we use precise pose information for each image. This is made possible by leveraging the nice functionalities provided by the Habitat-sim platform and the high-quality geometric and semantic information offered by the Gibson dataset. Specifically, we enable the simulation of pinhole cameras and stereo cameras within the environments, allowing for the acquisition of both RGB and depth information from these sensors.</p>
      <div style="display: flex; justify-content: center;">
        <img src="static/Gibson.png" alt="Gibson dataset" width="800" height="300">
      </div>
      <p>2. HM3D: The same dataset generation process can be applied to other compatible datasets such as HM3D and Replica. In this work, we generate the second half of Co-VisiON dataset from the HM3D. HM3D employs both pinhole and stereo cameras to capture images and provides mesh files. One significant difference between Gibson and HM3D lies in the availability of floor heights necessary for scene construction. While Gibson automatically encodes floor heights in its metadata, HM3D lacks a direct extraction method. Instead, we determine floor heights by sampling and clustering Y-axis values from pose data. With these heights established, we’ve created co-visibility graph annotations for 755 scenes in HM3D, each with clear boundaries between floors.</p>
        <p>Supported Features:</p>
        <ul>
          <li>3D scans of indoor/outdoor spaces (with built-in support for HM3D, MatterPort3D, Gibson, Replica, and other datasets)</li>
          <li>CAD models of spaces and piecewise-rigid objects (e.g. ReplicaCAD, YCB, Google Scanned Objects)</li>
          <li>Configurable sensors (RGB-D cameras, egomotion sensing)</li>
          <li>Robots described via URDF (mobile manipulators like Fetch, fixed-base arms like Franka, quadrupeds like AlienGo)</li>
          <li>Rigid-body mechanics (via Bullet)</li>
        </ul>
      <p>These two sources will provide us with high-quality, realistic, and diverse data, thereby enabling a comprehensive investigation into spatial reasoning for RGB image matching. The combination of these datasets will also offer the opportunity to validate our method across a broad spectrum of scenarios, further establishing the generalizability and robustness of our proposed solution.</p>
      <p>Dataset will be available after official publication</p>
  </div>


  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
</body>
</html>
