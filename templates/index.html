<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="static/ai4ce.png" type="image/x-icon">
  <title>Spatial_reasoning</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <style>
    /* 自定义CSS样式 */
    body {
      background-color: #f8f9fa; /* 设置背景色为浅灰色 */
    }

    .navbar-custom {
      background-color: #57068c; /* NYU紫色 */
    }

    .navbar-custom .navbar-brand {
      color: white;
      font-size: 24px;
    }

    .navbar-custom .navbar-nav .nav-link {
      color: white;
      font-size: 18px;
    }

    .navbar-custom .navbar-nav .nav-link:hover,
    .navbar-custom .navbar-nav .nav-link.active {
      background-color: #8e24aa; /* 鼠标悬停时和当前活动菜单项的背景色 */
    }

    .container {
      margin-top: 40px;
      background-color: white; /* 设置内容区域背景色为白色 */
      padding: 20px;
      border-radius: 10px; /* 添加圆角边框 */
      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1); /* 添加阴影效果 */
    }

    .container h2,
    .container h3,
    .container p {
      margin-bottom: 20px; /* 添加段落间距 */
    }
  </style>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      var currentPage = location.pathname;
      var navLinks = document.querySelectorAll(".navbar-nav .nav-link");
      navLinks.forEach(function(link) {
        var linkHref = link.getAttribute("href");
        if (linkHref && (currentPage === linkHref || (currentPage === '/' && linkHref === '/index'))) {
          link.classList.add("active");
        } else {
          link.classList.remove("active");
        }
      });
    });
  </script>
</head>
<body>
  <!-- 导航栏 -->
  <nav class="navbar navbar-expand-lg navbar-custom">
    <a class="navbar-brand" href="#">
      <img src="static/nyu-logo.jpg" alt="NYU Logo" height='60' width='60' class="d-inline-block align-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav"
      aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="/index">ABOUT</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/data">DATA</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/tutorial">TUTORIAL</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/methods">METHODS</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/ai4ce/Spatial_reasoning/tree/main">DOWNLOAD</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/aboutus">ABOUT US</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/visualize">VISUALIZE</a>
        </li>
      </ul>
    </div>
  </nav>

  <!-- 自定义主页内容 -->
  <div class="container">
    <h2>Abstract:</h2>
    <p>Humans possess amazing spatial reasoning abilities to analyze the co-visibility, i.e., the sharing of field-of-view, of a few images that are sparsely distributed in a complex scene. The large difference and the partial observability in those sparse views pose significant challenges to existing 3D vision techniques for pairwise image- or feature-matching. A more holistic reasoning approach that jointly analyzes the whole set of images is needed to solve this problem, which is of great importance in 3D vision, embodied intelligence, and robotics. Thus, we propose the Co-Visibility reasONing (Co-VisiON) task and curate an open-source dataset to facilitate the study of this problem. We develop a method to automatically sample sparse image sets and their co-visibilities from photorealistic indoor simulation environments, allowing us to quickly generate a large-scale dataset. We also manually annotate co-visibilities on the generated sets for comparisons. We benchmark various baselines on this dataset, including feature matching, visual place recognition (VPR), supervised or contrastive learning, and Large Vision Language Models (VLMs), among others. Their unsatisfactory performance highlights the challenges and calls for more studies in the community.</p>

    <h4>1. Task Definition:</h4>
    <p>
        The task involves constructing a co-visibility graph <em>G</em> from a sparse set of images
        <em>I = {I<sub>1</sub>, I<sub>2</sub>, ..., I<sub>n</sub>}</em> representing different views of a scene, such as a house.
        In this graph, images act as nodes. An edge connects two nodes if their corresponding images have co-visibility.
        For each pair of images <em>(I<sub>i</sub>, I<sub>j</sub>)</em>, an algorithm outputs a value <em>d<sub>ij</sub></em>
        indicating the degree of co-visibility. If <em>d<sub>ij</sub> > τ</em>, where τ is a predefined threshold,
        the images are considered to have overlapping views and are connected in the graph.
        The adjacency matrix <em>A</em> of the graph is then constructed, with each entry <em>a<sub>ij</sub></em>
        indicating the presence of co-visible areas between <em>I<sub>i</sub></em> and <em>I<sub>j</sub></em>.
    </p>
    <h4>2. Baseline Methods:</h4>
    <ul>
        <li>
            <strong>Supervised Classification:</strong> Utilizes classic image classification methods like ViT, VGG, and ResNet. These models extract image representations and use a classification head to predict the likelihood of image pairs being connected in the co-visibility graph. The models are fine-tuned with annotated data, and IOU scores are computed for various threshold values to calculate the AUC.
        </li>
        <li>
            <strong>Visual Place Recognition (VPR):</strong> Employs NetVLAD as the VPR baseline. This approach is similar to supervised classification, with a classification head predicting the probability of a view connection.
        </li>
        <li>
            <strong>Image Feature Matching:</strong> Uses SuperGlue with pretrained weights for image feature matching. Additionally, a SIFT+RANSAC method is employed as a traditional matching baseline. Thresholds are set based on the maximum matches detected.
        </li>
        <li>
            <strong>Contrastive Learning Method:</strong> Adapts the SimCLR model, coupled with the InfoNCE loss, for this task. Image pairs connected in the ground truth co-visibility graph are treated as positive instances. ResNet is used as the backbone, and pairwise Euclidean distances are recorded for thresholding and evaluation.
        </li>
        <li>
            <strong>In-context Learning with a Large Language Model:</strong> Designs an in-context learning baseline using GPT-4. Image descriptions are generated using a pretrained vision language model, and GPT-4 is used to summarize the context and predict co-visibility scores. The scores are normalized for thresholding and evaluation.
        </li>
    </ul>
    <h4>3. Dataset Collection:</h4>
    <p>
        The dataset for the co-visibility graph task is generated using the Habitat-sim platform, sampling scenes from the Gibson dataset which includes 3D models of various indoor environments. Images are captured at specific positions and orientations to maximize unseen area coverage while minimizing the total image count. The dataset comprises an average of 14 images per scenario across 86 different scenes, totaling 130 floors.
    </p>

    <h4>4. Co-Visibility Graph Annotation:</h4>
    <p>
        The annotation process involves both automatic and human methods. Automatic annotation uses a camera placement strategy and a progressive candidate selection method. Camera locations are chosen near walls or furniture, emulating real-world photography habits, and images are selected based on a scoring method that balances scene coverage and overlap. Human annotation involves trained annotators using a custom GUI to label image pairs based on shared objects, object continuity, sub-scene relationships, and featureless surfaces. This process includes cross-validation to ensure accuracy and robustness. Both the automatically and manually labeled datasets will be released for future research.
    </p>
    <div style="display: flex; justify-content: center;">
      <img src="static/pipeline.png" alt="Dataset generation Pipeline" width="800" height="500">
    </div>

    <h4>3. Confidence Score:</h4>
    <p>It calculates a confidence score for a candidate location within an environment, guiding a genetic algorithm-based navigation strategy. The score reflects the potential informativeness of moving to the candidate location, factoring in the expected observations and their overlap with existing maps, considering the relative positions of the candidate with saved poses. This score helps the algorithm to determine the next best location to explore based on current knowledge, ultimately returning the highest confidence candidate pose and its related elements.</p>

    <h4>4. Contributions:</h4>
    <p>The project contributions are threefold:</p>

      <p>     a.The construction of a reason dataset, which serves as the foundation for this research.</p>

      <p>     b.A comprehensive analysis of multiple baselines on both synthetic and real data sets. This comparative study would provide a clear understanding of the strengths and weaknesses of each method.</p>

      <p>     c.A simple yet innovative method to address the problem of long baseline spatial reasoning for RGB image matching. The novelty lies in the way spatial relations are inferred between distantly related images, contributing significantly to the field of image processing and computer vision.</p>

    <h4>5. Pipeline:</h4>
    <div style="display: flex; justify-content: center;">
      <img src="static/pipeline2.png" alt="Overall Pipeline" width="800" height="450">
    </div>
  </div>

  <!-- 添加所需的JavaScript文件 -->
  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
</body>
</html>
