
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="icon" href="static/unknown.png" type="image/x-icon">
  <title>Co-VisiON</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <style>
    body {
      background-color: #f8f9fa;
    }

    .navbar-custom {
      background-color: #57068c;
    }

    .navbar-custom .navbar-brand {
      color: white;
      font-size: 24px;
    }

    .navbar-custom .navbar-nav .nav-link {
      color: white;
      font-size: 18px;
    }

    .navbar-custom .navbar-nav .nav-link:hover,
    .navbar-custom .navbar-nav .nav-link.active {
      background-color: #8e24aa;
    }

    .container {
      margin-top: 40px;
      background-color: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.1);
    }

    .container h2,
    .container h3,
    .container p {
      margin-bottom: 20px;
    }
  </style>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      var currentPage = location.pathname;
      var navLinks = document.querySelectorAll(".navbar-nav .nav-link");
      navLinks.forEach(function(link) {
        var linkHref = link.getAttribute("href");
        if (linkHref && (currentPage === linkHref || (currentPage === '/' && linkHref === '/index'))) {
          link.classList.add("active");
        } else {
          link.classList.remove("active");
        }
      });
    });
    function showAlert() {
        alert("Unable to provide download link till Official published");
    }
  </script>
</head>
<body>
  <nav class="navbar navbar-expand-lg navbar-custom">
    <a class="navbar-brand" href="#">
      <img src="static/unknown-logo.jpg" alt="unknown Logo" height='60' width='60' class="d-inline-block align-top">
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav"
      aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse justify-content-center" id="navbarNav">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="/index">ABOUT</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/data">DATA</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/tutorial">TUTORIAL</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="/methods">METHODS</a>
        </li>
        <li class="nav-item">
            <a class="nav-link" href="#" onclick="showAlert()">DOWNLOAD</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/aboutus">ABOUT US</a>
        </li>
        <li class="nav-item">
          <a class="nav-link active" href="/visualize">VISUALIZE</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="container">
    <h2>Abstract:</h2>
    <p>Humans possess amazing spatial reasoning abilities to analyze the co-visibility, i.e., the sharing of field-of-view, of a few images that are sparsely distributed in a complex scene. The large difference and the partial observability in those sparse views pose significant challenges to existing 3D vision techniques for pairwise image or feature-matching. A holistic approach that analyzes the entire set of images is essential for progress in 3D vision, embodied intelligence, and robotics. Thus, we propose the Co-Visibility reasONing (Co-VisiON) task and curate an open-source dataset to facilitate the study of this problem. We develop an automated method to sample sparse image sets and their co-visibilities from photorealistic indoor simulation environments, allowing us to quickly generate a large-scale dataset from Gibson and HM3D environments. Additionally, we cross-validate the labeling of our dataset and test the trained model on our manually annotated co-visibilities, both on the generated and real-world sets. To solve this task, we benchmark various baselines on this dataset, including feature matching, visual place recognition (VPR), supervised or contrastive learning, and Large Vison Language Models (VLMs), among others. Subsequently, we assessed the applicability of our dataset and Co-VisiON graph on downstream challenges, including 3D reconstruction, cross-view completion, and Sim2Real. Our baseline benchmarks and downstream tasks highlight the challenges, importance, and calls for more studies in the community.</p>
    <div style="display: flex; justify-content: center;">
      <img src="static/figure1_teaser.png" alt="Overall Pipeline" width="800" height="450">
    </div>
    <h4>1. Task Definition:</h4>
    <p>
        If two images have enough overlapping fields of view (FOV), they should be connected in the binary co-visibility graph.Traditional pairwise feature-matching techniques fall short of this task when facing diverse viewpoints with significant angular and spatial disparities. A more holistic analysis of the whole set of images jointly is required to tackle this challenging spatial reasoning task.
    </p>
    <h4>2. Baseline Methods:</h4>
    <ul>
        <li>
            <strong>Supervised Classification:</strong> Utilizes classic image classification methods like ViT, VGG, and ResNet. These models extract image representations and use a classification head to predict the likelihood of image pairs being connected in the co-visibility graph. The models are fine-tuned with annotated data, and IOU scores are computed for various threshold values to calculate the AUC.
        </li>
        <li>
            <strong>Visual Place Recognition (VPR):</strong> Employs NetVLAD as the VPR baseline. This approach is similar to supervised classification, with a classification head predicting the probability of a view connection.
        </li>
        <li>
            <strong>Image Feature Matching:</strong> Uses SuperGlue with pretrained weights for image feature matching. Additionally, a SIFT+RANSAC method is employed as a traditional matching baseline. Thresholds are set based on the maximum matches detected.
        </li>
        <li>
            <strong>Contrastive Learning Method:</strong> Adapts the SimCLR model, coupled with the InfoNCE loss, for this task. Image pairs connected in the ground truth co-visibility graph are treated as positive instances. ResNet is used as the backbone, and pairwise Euclidean distances are recorded for thresholding and evaluation.
        </li>
        <li>
            <strong>In-context Learning with a Large Language Model:</strong> Designs an in-context learning baseline using GPT-4. Image descriptions are generated using a pretrained vision language model, and GPT-4 is used to summarize the context and predict co-visibility scores. The scores are normalized for thresholding and evaluation.
        </li>
    </ul>
    <h4>3. Dataset Collection:</h4>
    <p>
        The dataset for the co-visibility graph task is generated using the Habitat-sim platform, sampling scenes from the Gibson dataset which includes 3D models of various indoor environments. Images are captured at specific positions and orientations to maximize unseen area coverage while minimizing the total image count. The dataset comprises an average of 14 images per scenario across 86 different scenes, totaling 130 floors.
    </p>

    <h4>4. Co-Visibility Graph Annotation:</h4>
    <p>
        The annotation process involves both automatic and human methods. Automatic annotation uses a camera placement strategy and a progressive candidate selection method. Camera locations are chosen near walls or furniture, emulating real-world photography habits, and images are selected based on a scoring method that balances scene coverage and overlap. Human annotation involves trained annotators using a custom GUI to label image pairs based on shared objects, object continuity, sub-scene relationships, and featureless surfaces. This process includes cross-validation to ensure accuracy and robustness. Both the automatically and manually labeled datasets will be released for future research.
    </p>
    <div style="display: flex; justify-content: center;">
      <img src="static/pipeline2.png" alt="Overall Pipeline" width="800" height="450">
    </div>

    <h4>5. Conclusion:</h4>
    <p><strong>Limitation:</strong> The current co-visibility reasoning task presents several notable limitations. For the dataset, the scenes are restricted to the Gibson and HM3D environment and the human annotations are limited. With the proposed data collection pipeline and website, our future works will continue to enlarge and diversify the dataset scenes with other sources including from the real world. For the methods, we only experimented with predominant baselines from relevant tasks and their

 performance leaves significant room for improvement. Future works will consider developing new methods that can reason with the entire images set from a scene altogether or conducting imitation learning following the human co-visibility reasoning process.</p>
    <p><strong>Summary:</strong> This paper proposes the Co-Visibility reasONing (Co-VisiON) task, where models are presented with sparse image sets taken in an indoor environment, and are asked to reason their co-visibilities relationships by figuring out the view connectivity and predicting a binary co-visibility graph. We curated a dataset designated to this task containing both human and automated annotations and conducted comprehensive experiments to show the Co-VisiON task challenges predominant image classification and matching baselines. We hope the Co-VisiON task can facilitate future research in embodied scene understanding.</p>
  </div>

  <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
</body>
</html>
